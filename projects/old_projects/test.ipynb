{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate old project pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = os.listdir(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "layout: project\n",
      "permalink: aerial-lidar\n",
      "title: Aerial Lidar\n",
      "tag: aerial-lidar\n",
      "enable_nav: false\n",
      "---\n",
      "    \n",
      "---\n",
      "layout: project\n",
      "permalink: angry-birds\n",
      "title: Angry Birds\n",
      "tag: angry-birds\n",
      "enable_nav: false\n",
      "---\n",
      "    \n",
      "---\n",
      "layout: project\n",
      "permalink: auto-airplanes\n",
      "title: Auto Airplanes\n",
      "tag: auto-airplanes\n",
      "enable_nav: false\n",
      "---\n",
      "    \n",
      "---\n",
      "layout: project\n",
      "permalink: auto-copters\n",
      "title: Auto Copters\n",
      "tag: auto-copters\n",
      "enable_nav: false\n",
      "---\n",
      "    \n",
      "---\n",
      "layout: project\n",
      "permalink: aye-aye\n",
      "title: Aye Aye\n",
      "tag: aye-aye\n",
      "enable_nav: false\n",
      "---\n",
      "    \n",
      "---\n",
      "layout: project\n",
      "permalink: bird-net-tracker\n",
      "title: Bird Net Tracker\n",
      "tag: bird-net-tracker\n",
      "enable_nav: false\n",
      "---\n",
      "    \n",
      "---\n",
      "layout: project\n",
      "permalink: burrowing-owls\n",
      "title: Burrowing Owls\n",
      "tag: burrowing-owls\n",
      "enable_nav: false\n",
      "---\n",
      "    \n",
      "---\n",
      "layout: project\n",
      "permalink: condor-cam\n",
      "title: Condor Cam\n",
      "tag: condor-cam\n",
      "enable_nav: false\n",
      "---\n",
      "    \n",
      "---\n",
      "layout: project\n",
      "permalink: del-dios-monitoring\n",
      "title: Del Dios Monitoring\n",
      "tag: del-dios-monitoring\n",
      "enable_nav: false\n",
      "---\n",
      "    \n",
      "---\n",
      "layout: project\n",
      "permalink: elephant-monitoring\n",
      "title: Elephant Monitoring\n",
      "tag: elephant-monitoring\n",
      "enable_nav: false\n",
      "---\n",
      "    \n",
      "---\n",
      "layout: project\n",
      "permalink: Intelligent-camera-trap\n",
      "title: Intelligent Camera Trap\n",
      "tag: Intelligent-camera-trap\n",
      "enable_nav: false\n",
      "---\n",
      "    \n",
      "---\n",
      "layout: project\n",
      "permalink: maya-achaeology\n",
      "title: Maya Achaeology\n",
      "tag: maya-achaeology\n",
      "enable_nav: false\n",
      "---\n",
      "    \n",
      "---\n",
      "layout: project\n",
      "permalink: Nautical Archaeology\n",
      "title: Nautical Archaeology\n",
      "tag: Nautical Archaeology\n",
      "enable_nav: false\n",
      "---\n",
      "    \n",
      "---\n",
      "layout: project\n",
      "permalink: openrov\n",
      "title: Openrov\n",
      "tag: openrov\n",
      "enable_nav: false\n",
      "---\n",
      "    \n",
      "---\n",
      "layout: project\n",
      "permalink: searching-for-harpy-eagles\n",
      "title: Searching For Harpy Eagles\n",
      "tag: searching-for-harpy-eagles\n",
      "enable_nav: false\n",
      "---\n",
      "    \n",
      "---\n",
      "layout: project\n",
      "permalink: stabilized-aerial-camera-platform\n",
      "title: Stabilized Aerial Camera Platform\n",
      "tag: stabilized-aerial-camera-platform\n",
      "enable_nav: false\n",
      "---\n",
      "    \n",
      "---\n",
      "layout: project\n",
      "permalink: terrestrial-vechicle\n",
      "title: Terrestrial Vechicle\n",
      "tag: terrestrial-vechicle\n",
      "enable_nav: false\n",
      "---\n",
      "    \n",
      "---\n",
      "layout: project\n",
      "permalink: tiger-tracking\n",
      "title: Tiger Tracking\n",
      "tag: tiger-tracking\n",
      "enable_nav: false\n",
      "---\n",
      "    \n",
      "---\n",
      "layout: project\n",
      "permalink: tunnel-bot\n",
      "title: Tunnel Bot\n",
      "tag: tunnel-bot\n",
      "enable_nav: false\n",
      "---\n",
      "    \n",
      "---\n",
      "layout: project\n",
      "permalink: underwater-cave-imaging\n",
      "title: Underwater Cave Imaging\n",
      "tag: underwater-cave-imaging\n",
      "enable_nav: false\n",
      "---\n",
      "    \n",
      "---\n",
      "layout: project\n",
      "permalink: vaquita-monitoring\n",
      "title: Vaquita Monitoring\n",
      "tag: vaquita-monitoring\n",
      "enable_nav: false\n",
      "---\n",
      "    \n",
      "---\n",
      "layout: project\n",
      "permalink: wolf-monitoring\n",
      "title: Wolf Monitoring\n",
      "tag: wolf-monitoring\n",
      "enable_nav: false\n",
      "---\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "for project in projects:\n",
    "    if project==\"test.ipynb\": continue\n",
    "    title = \" \".join(project.split(\"-\")).title()\n",
    "    file_format = f\"\"\"---\n",
    "layout: project\n",
    "permalink: {project}\n",
    "title: {title}\n",
    "tag: {project}\n",
    "enable_nav: false\n",
    "---\n",
    "    \"\"\"\n",
    "    with open(f'./{project}/{project}.md', 'w') as f:\n",
    "        f.write(file_format)\n",
    "    print(file_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# auto formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = requests.get(\"https://e4e.ucsd.edu/past-projects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(request.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = soup.find(id=\"primary\").find_all(\"img\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "![]({{assets/projects/_old_projects/lubrak-village-frontal-ps3_edited-e1654024061974.jpg | absolute_url}}\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../assets/projects/_old_projects/lubrak-village-frontal-ps3_edited-e1654024061974.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m data \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(src)\u001b[38;5;241m.\u001b[39mcontent \n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Opening a new file named img with extension .jpg \u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# This file would store the data of the image file \u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43masset_folder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Storing the image data inside the data variable to the file \u001b[39;00m\n\u001b[0;32m     16\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(data) \n",
      "File \u001b[1;32mc:\\Users\\seanh\\miniconda3\\envs\\asid\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../assets/projects/_old_projects/lubrak-village-frontal-ps3_edited-e1654024061974.jpg'"
     ]
    }
   ],
   "source": [
    "asset_folder = \"../../assets/projects/_old_projects/\"\n",
    "asset_folder_md = \"assets/projects/_old_projects/\"\n",
    "for image in images:\n",
    "    \n",
    "    src = image[\"src\"]\n",
    "    filename = src.split(\"/\")[-1]\n",
    "    print('![]({{' +asset_folder_md + filename+ ' | absolute_url}}') \n",
    "\n",
    "    data = requests.get(src).content \n",
    "    \n",
    "    # Opening a new file named img with extension .jpg \n",
    "    # This file would store the data of the image file \n",
    "    f = open(asset_folder + filename,'wb') \n",
    "    \n",
    "    # Storing the image data inside the data variable to the file \n",
    "    f.write(data) \n",
    "    f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(asset_folder, og_webpage):\n",
    "    request = requests.get(og_webpage)\n",
    "    soup = BeautifulSoup(request.text)\n",
    "    images = soup.find(id=\"primary\").find_all(\"img\")\n",
    "    \n",
    "    asset_folder_md = asset_folder.replace(\"../\", \"\")\n",
    "    for image in images:\n",
    "        src = image[\"src\"]\n",
    "        filename = src.split(\"/\")[-1]\n",
    "        print('![]({{' +asset_folder_md + filename+ ' | absolute_url}}') \n",
    "\n",
    "        data = requests.get(src).content \n",
    "        \n",
    "        # Opening a new file named img with extension .jpg \n",
    "        # This file would store the data of the image file \n",
    "        f = open(asset_folder + filename,'wb') \n",
    "        \n",
    "        # Storing the image data inside the data variable to the file \n",
    "        f.write(data) \n",
    "        f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "![]({{assets/projects/_old_projects/lubrak-village-frontal-ps3_edited-e1654024061974.jpg | absolute_url}}\n",
      "![]({{assets/projects/_old_projects/aye-aye-baby_250x250.jpg | absolute_url}}\n",
      "![]({{assets/projects/_old_projects/burronw_owl_thumbnail.png | absolute_url}}\n",
      "![]({{assets/projects/_old_projects/project_mayaarchaeology.jpg | absolute_url}}\n",
      "![]({{assets/projects/_old_projects/tahoe_underwater.jpg | absolute_url}}\n",
      "![]({{assets/projects/_old_projects/project_angrybirds4.jpg | absolute_url}}\n",
      "![]({{assets/projects/_old_projects/elephant_monitoring_thumb.jpg | absolute_url}}\n",
      "![]({{assets/projects/_old_projects/tiger_tracker_thumb.jpg | absolute_url}}\n",
      "![]({{assets/projects/_old_projects/wolf_tracking_thumb.jpg | absolute_url}}\n",
      "![]({{assets/projects/_old_projects/bird_nest_thumb.jpg | absolute_url}}\n",
      "![]({{assets/projects/_old_projects/project_deldios.jpg | absolute_url}}\n",
      "![]({{assets/projects/_old_projects/aerial_lidar_thumb.jpg | absolute_url}}\n",
      "![]({{assets/projects/_old_projects/autonomous_plane_thumb.jpg | absolute_url}}\n",
      "![]({{assets/projects/_old_projects/copter_thumb.jpg | absolute_url}}\n",
      "![]({{assets/projects/_old_projects/project_condorcam.png | absolute_url}}\n",
      "![]({{assets/projects/_old_projects/project_balloon.jpg | absolute_url}}\n",
      "![]({{assets/projects/_old_projects/project_cameratrap.jpg | absolute_url}}\n",
      "![]({{assets/projects/_old_projects/project_vehicle.jpg | absolute_url}}\n",
      "![]({{assets/projects/_old_projects/project_tunnelbot.jpg | absolute_url}}\n",
      "![]({{assets/projects/_old_projects/openrov.jpg | absolute_url}}\n",
      "![]({{assets/projects/_old_projects/underwater_cave_mapping_thumb.jpg | absolute_url}}\n",
      "![]({{assets/projects/_old_projects/vaquita_monitoring.jpg | absolute_url}}\n",
      "![]({{assets/projects/_old_projects/harpy-eagle.jpg | absolute_url}}\n"
     ]
    }
   ],
   "source": [
    "download_images(\"../../assets/projects/_old_projects/\", \"https://e4e.ucsd.edu/past-projects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal: Automate Page Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\seanh\\miniconda3\\envs\\asid\\lib\\site-packages\\ipykernel\\kernelbase.py:1270\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1268\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1270\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\seanh\\miniconda3\\envs\\asid\\lib\\site-packages\\ipykernel\\kernelbase.py:1313\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1311\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1313\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "input(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_markdown(asset_folder, og_webpage):\n",
    "    request = requests.get(og_webpage)\n",
    "    soup = BeautifulSoup(request.text)\n",
    "    images = soup.find(id=\"primary\").find_all(\"img\")\n",
    "    \n",
    "    asset_folder_md = asset_folder.replace(\"../\", \"\")\n",
    "    for image in images:\n",
    "        src = image[\"src\"]\n",
    "        filename = src.split(\"/\")[-1]\n",
    "        print('![]({{' +asset_folder_md + filename+ ' | absolute_url}}') \n",
    "\n",
    "        data = requests.get(src).content \n",
    "        \n",
    "        # Opening a new file named img with extension .jpg \n",
    "        # This file would store the data of the image file \n",
    "        f = open(asset_folder + filename,'wb') \n",
    "        \n",
    "        # Storing the image data inside the data variable to the file \n",
    "        f.write(data) \n",
    "        f.close()\n",
    "#download_images(\"../../assets/projects/_old_projects/aye-aye\", \"https://e4e.ucsd.edu/aye-aye-sleep-monitoring\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = requests.get(\"https://e4e.ucsd.edu/aye-aye-sleep-monitoring\")\n",
    "soup = BeautifulSoup(request.text)\n",
    "soup = soup.find(id=\"primary\")\n",
    "soup.find(tag=\"img\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import Tag\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<div class=\"content-area\" id=\"primary\">',\n",
       " '<main class=\"site-main\" id=\"main\" role=\"main\">',\n",
       " '<!-- Main project title -->',\n",
       " '<article class=\"post-5800 page type-page status-publish hentry\" id=\"post-5800\">',\n",
       " '<header class=\"entry-header\">',\n",
       " '<h1 class=\"entry-title\">Aye-Aye Sleep Monitoring</h1>',\n",
       " '<!-- Added subnavigation bar -->',\n",
       " '<nav class=\"page-navigation\" id=\"page-navigation\" role=\"navigation\">',\n",
       " '<!--<h1 class=\"menu-toggle\">Menu</h1>-->',\n",
       " '<div class=\"screen-reader-text skip-link\">',\n",
       " '<a href=\"#content\" title=\"Skip to content\">Skip to content</a>',\n",
       " '</div>',\n",
       " '</nav><!-- #page-navigation -->',\n",
       " '</header><!-- .entry-header -->',\n",
       " '<!-- Added ability for only posts from a single category to be displayed on a page titled \"Project Updates\" -->',\n",
       " '<div class=\"single-entry-content entry-content\">',\n",
       " '<figure class=\"wp-block-image size-large\"><img alt=\"\" decoding=\"async\" src=\"https://sdzwildlifeexplorers.org/sites/default/files/2019-12/aye-aye-baby.jpg\"><figcaption class=\"wp-element-caption\">Baby aye-aye courtesy of SD Zoo Wildlife Alliance</figcaption></img></figure>',\n",
       " '<p>One challenge in caring for animals is being able to monitor the animal’s behavior, even when caretakers are not on duty.  This is especially challenging in nocturnal species such as the aye-aye, particularly when they are in environments with significant anthropogenic stimuli, such as at the San Diego Zoo in Balboa Park.</p>',\n",
       " '<p>We want to develop a system for intelligently monitoring animals 24/7 so that scientists and caretakers can easily locate the relevant and interesting data instead of poring over hours of irrelevant data.  This takes on several aspects – reliable long-life embedded sensor networks, big data management, and machine learning.</p>',\n",
       " '<figure class=\"wp-block-image size-large\"><img alt=\"\" class=\"wp-image-5934\" decoding=\"async\" fetchpriority=\"high\" height=\"604\" sizes=\"(max-width: 1074px) 100vw, 1074px\" src=\"https://e4e.ucsd.edu/wp-content/uploads/2023.01.09.on-box-render-1074x604.png\" srcset=\"https://e4e.ucsd.edu/wp-content/uploads/2023.01.09.on-box-render-1074x604.png 1074w, https://e4e.ucsd.edu/wp-content/uploads/2023.01.09.on-box-render-350x197.png 350w, https://e4e.ucsd.edu/wp-content/uploads/2023.01.09.on-box-render-768x432.png 768w, https://e4e.ucsd.edu/wp-content/uploads/2023.01.09.on-box-render-1536x864.png 1536w, https://e4e.ucsd.edu/wp-content/uploads/2023.01.09.on-box-render.png 1920w\" width=\"1074\"><figcaption class=\"wp-element-caption\">On-Box Sensor Unit</figcaption></img></figure>',\n",
       " '<p>If you have any of the following skillsets and are interested in this project, please reach out to our staff engineer Nathan Hui (<a href=\"javascript:DeCryptX(\\'2p0t1i3x3l0@3x2e1t3g0.1f0d1v\\')\">nthui@ucsd.edu</a>)</p>',\n",
       " '<ul>',\n",
       " '<li>Linux systemd</li>',\n",
       " '<li>Networking</li>',\n",
       " '<li>Python and multithreading</li>',\n",
       " '<li>Digital camera/lighting/optics</li>',\n",
       " '<li>3D Printing</li>',\n",
       " '<li>Test and evaluation</li>',\n",
       " '<li>Systems engineering</li>',\n",
       " '</ul>',\n",
       " '</div><!-- .entry-content -->',\n",
       " '</article><!-- #post-## -->',\n",
       " '</main><!-- #main -->',\n",
       " '</div>']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = str(soup).split(\"\\n\")\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 class=\"entry-title\">Aye-Aye Sleep Monitoring</h1> ['Aye-Aye Sleep Monitoring']\n",
      "<!--<h1 class=\"menu-toggle\">Menu</h1>--> ['Menu']\n",
      "['https://sdzwildlifeexplorers.org/sites/default/files/2019-12/aye-aye-baby.jpg']\n",
      "<p>One challenge in caring for animals is being able to monitor the animal’s behavior, even when caretakers are not on duty.  This is especially challenging in nocturnal species such as the aye-aye, particularly when they are in environments with significant anthropogenic stimuli, such as at the San Diego Zoo in Balboa Park.</p>\n",
      "<p>We want to develop a system for intelligently monitoring animals 24/7 so that scientists and caretakers can easily locate the relevant and interesting data instead of poring over hours of irrelevant data.  This takes on several aspects – reliable long-life embedded sensor networks, big data management, and machine learning.</p>\n",
      "['https://e4e.ucsd.edu/wp-content/uploads/2023.01.09.on-box-render-1074x604.png']\n",
      "<p>If you have any of the following skillsets and are interested in this project, please reach out to our staff engineer Nathan Hui (<a href=\"javascript:DeCryptX('2p0t1i3x3l0@3x2e1t3g0.1f0d1v')\">nthui@ucsd.edu</a>)</p>\n",
      "=============================\n",
      "\n",
      "#Aye-Aye Sleep Monitoring\n",
      "#Menu\n",
      "![]({{assets/projects/_old_projects/aye-aye/aye-aye-baby.jpg | absolute_url}})\n",
      "One challenge in caring for animals is being able to monitor the animal’s behavior, even when caretakers are not on duty.  This is especially challenging in nocturnal species such as the aye-aye, particularly when they are in environments with significant anthropogenic stimuli, such as at the San Diego Zoo in Balboa Park.\n",
      "We want to develop a system for intelligently monitoring animals 24/7 so that scientists and caretakers can easily locate the relevant and interesting data instead of poring over hours of irrelevant data.  This takes on several aspects – reliable long-life embedded sensor networks, big data management, and machine learning.\n",
      "![]({{assets/projects/_old_projects/aye-aye/2023.01.09.on-box-render-1074x604.png | absolute_url}})\n",
      "If you have any of the following skillsets and are interested in this project, please reach out to our staff engineer Nathan Hui (<a href=\"javascript:DeCryptX('2p0t1i3x3l0@3x2e1t3g0.1f0d1v')\">nthui@ucsd.edu</a>)\n",
      " - Linux systemd\n",
      " - Networking\n",
      " - Python and multithreading\n",
      " - Digital camera/lighting/optics\n",
      " - 3D Printing\n",
      " - Test and evaluation\n",
      " - Systems engineering\n",
      "\n"
     ]
    }
   ],
   "source": [
    "asset_folder = \"../../assets/projects/_old_projects/aye-aye/\"\n",
    "asset_folder_md = asset_folder.replace(\"../\", \"\")\n",
    "file = str(soup).split(\"\\n\")\n",
    "\n",
    "new_file = \"\"\n",
    "for line in file:\n",
    "    if len(re.findall(r\"<h\\d\", line)) > 0:\n",
    "       print(line, re.findall(r\">([^<]+)<\", line))\n",
    "       new_line = \"#\" + (re.findall(r\">([^<]+)<\", line)[0]) # * int(line[2])\n",
    "       new_file += new_line\n",
    "    if line.startswith(\"<p\"):\n",
    "       print(line)\n",
    "       new_file += re.findall(r\"p>(.+)</p\", line)[0]\n",
    "    if line.startswith(\"<li\"):\n",
    "       new_file += \" - \" + re.findall(r\"li>(.+)</li\", line)[0]\n",
    "    if \"<img\" in line:\n",
    "       images = re.findall(r\"src=\\\"([/\\w.?\\-:_]+)\\\"\", line)\n",
    "       print(images)\n",
    "       for image in images:\n",
    "        src = image\n",
    "        filename = src.split(\"/\")[-1]\n",
    "        new_file += '![]({{\"' +asset_folder_md + filename+ '\" | absolute_url}})\\n' \n",
    "\n",
    "        data = requests.get(src).content \n",
    "        \n",
    "        # Opening a new file named img with extension .jpg \n",
    "        # This file would store the data of the image file \n",
    "        f = open(asset_folder + filename,'wb') \n",
    "        \n",
    "        # Storing the image data inside the data variable to the file \n",
    "        f.write(data) \n",
    "        f.close()\n",
    "       \n",
    "    new_file += \"\\n\"\n",
    "\n",
    "new_file = re.sub(r\"(\\n)+\", \"\\n\", new_file)\n",
    "print(\"=============================\")\n",
    "print(new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_markdown(asset_folder, og_webpage):\n",
    "    asset_folder_md = asset_folder.replace(\"../\", \"\")\n",
    "    \n",
    "    request = requests.get(og_webpage)\n",
    "    soup = BeautifulSoup(request.text)\n",
    "    soup = soup.find(id=\"primary\")\n",
    "    file = str(soup).split(\"\\n\")\n",
    "    \n",
    "    new_file = \"\"\n",
    "    for line in file:\n",
    "        try:\n",
    "            if len(re.findall(r\"<h\\d\", line)) > 0:\n",
    "                print(line, re.findall(r\">([^<]+)<\", line))\n",
    "                new_line = \"# \" + (re.findall(r\">([^<]+)<\", line)[0]) # * int(line[2])\n",
    "                new_file += new_line\n",
    "            if line.startswith(\"<p\"):\n",
    "                print(line)\n",
    "                new_file += re.findall(r\"p>(.+)</p\", line)[0]\n",
    "            if line.startswith(\"<li\"):\n",
    "                new_file += \" - \" + re.findall(r\"li>(.+)</li\", line)[0]\n",
    "            if \"<img\" in line:\n",
    "                images = re.findall(r\"src=\\\"([/\\w.?\\-:_]+)\\\"\", line)\n",
    "                print(images)\n",
    "                for image in images:\n",
    "                    src = image\n",
    "                    filename = src.split(\"/\")[-1]\n",
    "                    new_file += '![]({{\"' +asset_folder_md + filename+ '\" | absolute_url}})\\n' \n",
    "\n",
    "                    data = requests.get(src).content \n",
    "                    \n",
    "                    # Opening a new file named img with extension .jpg \n",
    "                    # This file would store the data of the image file \n",
    "                    f = open(asset_folder + filename,'wb') \n",
    "                    \n",
    "                    # Storing the image data inside the data variable to the file \n",
    "                    f.write(data) \n",
    "                    f.close()\n",
    "        except:\n",
    "            new_file += \"ERROR: \" + line \n",
    "            print(\"ERROR: \" + line)\n",
    "            pass\n",
    "        if \"iframe\" in line:\n",
    "            new_file += line    \n",
    "        new_file += \"\\n\"\n",
    "               \n",
    "\n",
    "    new_file = re.sub(r\"(\\n)+\", \"\\n\\n\", new_file)\n",
    "    return new_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 class=\"entry-title\">Aye-Aye Sleep Monitoring</h1> ['Aye-Aye Sleep Monitoring']\n",
      "<!--<h1 class=\"menu-toggle\">Menu</h1>--> ['Menu']\n",
      "['https://sdzwildlifeexplorers.org/sites/default/files/2019-12/aye-aye-baby.jpg']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../assets/projects/_old_projects/aye-aye/aye-aye-baby.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mconvert_to_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../../assets/projects/_old_projects/aye-aye/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://e4e.ucsd.edu/aye-aye-sleep-monitoring\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 30\u001b[0m, in \u001b[0;36mconvert_to_markdown\u001b[1;34m(asset_folder, og_webpage)\u001b[0m\n\u001b[0;32m     26\u001b[0m data \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(src)\u001b[38;5;241m.\u001b[39mcontent \n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Opening a new file named img with extension .jpg \u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# This file would store the data of the image file \u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43masset_folder\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Storing the image data inside the data variable to the file \u001b[39;00m\n\u001b[0;32m     33\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(data) \n",
      "File \u001b[1;32mc:\\Users\\seanh\\miniconda3\\envs\\asid\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../assets/projects/_old_projects/aye-aye/aye-aye-baby.jpg'"
     ]
    }
   ],
   "source": [
    "# convert_to_markdown( \"../../assets/projects/_old_projects/aye-aye/\", \"https://e4e.ucsd.edu/aye-aye-sleep-monitoring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 class=\"entry-title\">Maya Archaeology</h1> ['Maya Archaeology']\n",
      "<!--<h1 class=\"menu-toggle\">Menu</h1>--> ['Menu']\n",
      "<p><img alt=\"Scanning Masks in Maya Temples\" class=\"alignright\" decoding=\"async\" fetchpriority=\"high\" height=\"213\" src=\"https://c3.staticflickr.com/9/8522/29008736970_a393afb9a5_n.jpg\" width=\"320\"><span style=\"font-size: inherit;\">In the dense jungle canopies of Central America and the Yucatan, much of what remains of the Maya civilization remains hidden from view. While important archaeological sites have been discovered and are now iconic tourist destinations, an understanding of how the Maya used the land – the extent of their settlements, the organization of towns and spheres of influences – is yet unknown. Currently, remote sensing technologies that can see through the thick canopy exist, but they are prohibitively expensive. Fortunately, the convergence of aerial drones and light-weight advanced sensor packages such as LiDAR, have just now opened the door to creating the disruptive technology needed to explore the hidden secrets of the Maya. One goal of this project is to assemble a remote sensing device using drone technology for the jungle.</span></img></p>\n",
      "['https://c3.staticflickr.com/9/8522/29008736970_a393afb9a5_n.jpg']\n",
      "<p>Already discovered Maya ruins are hidden under dense jungle. As a consequence, many sites that are not popular tourist destinations, yet instrumental in understanding Maya culture, are seen by very few people. Items found during excavation are typically claimed for preservation in museums. Large exhibits such as temple structures, by virtue of being buildings, cannot be demonstrated to a wider audience. Our goal is to change how archaeological finds are shared by exploring digital methods for documentation and visualization.</p>\n",
      "<p>This project is divided into 3 main parts:</p>\n",
      "<h2><strong>Aerial Surveys</strong></h2> ['Aerial Surveys']\n",
      "<h2><strong>Tunnel Mapping</strong></h2> ['Tunnel Mapping']\n",
      "<p>The Tunnel Mapping project aims to lower the cost of digital documentation by experimenting with data collection methods. These methods include stereo-panoramic cameras, LiDAR, and experimental remote sensing systems based on the Microsoft Kinect camera or the Google Tango tablet. In prior expeditions to Guatemala, we brought a ground-based LiDAR system for high resolution scans of these large excavated temples. One method we are particularly excited about is Structure from Motion (SfM), a low cost technique for generating 3D models using photos from a traditional camera. This group is seeking highly motivated individuals to build data collection infrastructure for expeditions.</p>\n",
      "<p><iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/pTD_uXEwNHw\" width=\"540\"></iframe></p>\n",
      "<p>Below, is a fly-through video created from a composite point cloud generated from 50 LIDAR scans.</p>\n",
      "<p>This video is of a point cloud generated by taking pictures of a stucco mask inside of an excavated temple on the site.</p>\n",
      "<h2><strong>Virtual Reality Visualization</strong></h2> ['Virtual Reality Visualization']\n",
      "<p style=\"text-align: center;\"><strong><em><a href=\"https://sites.google.com/ucsd.edu/maya-archeology-xr/home\">Maya Archaeology VR Website</a></em></strong></p>\n",
      "ERROR: <p style=\"text-align: center;\"><strong><em><a href=\"https://sites.google.com/ucsd.edu/maya-archeology-xr/home\">Maya Archaeology VR Website</a></em></strong></p>\n",
      "<p>We seek to expand distribution by creating immersive visualizations of the many 3D models that we have collected over the years. We are looking for any person motivated by VR environments, video game creation, 3D modeling, and 3D point cloud manipulation.</p>\n",
      "<p><script src=\"https://player.vimeo.com/api/player.js\"></script></p>\n",
      "<p>Visualization of Structure M7-1 in El Zotz Guatemala.<br/>\n",
      "ERROR: <p>Visualization of Structure M7-1 in El Zotz Guatemala.<br/>\n",
      "<p>Created using Point clouds from the Faro Focus 3D 120, Lieca BLK360.<br/>\n",
      "ERROR: <p>Created using Point clouds from the Faro Focus 3D 120, Lieca BLK360.<br/>\n",
      "['https://e4e.ucsd.edu/wp-content/uploads/SpiderMonkeyBowlUnrealEngine-350x255.png']\n",
      "<h3 style=\"text-align: left;\">Approach</h3> ['Approach']\n",
      "<h4 class=\"CjVfdc CJIdie\" style=\"text-align: left;\"><span style=\"text-decoration: underline;\">Data Acquisition through Remote Scanning</span></h4> ['Data Acquisition through Remote Scanning']\n",
      "ERROR: <li class=\"TYR86d wXCUfe zfr3Q\" dir=\"ltr\">\n",
      "<p class=\"CDt4Ke zfr3Q\" dir=\"ltr\">Terrestrial Light Detection and Ranging (LiDAR)</p>\n",
      "ERROR: <p class=\"CDt4Ke zfr3Q\" dir=\"ltr\">Terrestrial Light Detection and Ranging (LiDAR)</p>\n",
      "ERROR: <li class=\"TYR86d wXCUfe zfr3Q\" dir=\"ltr\">\n",
      "<p class=\"CDt4Ke zfr3Q\" dir=\"ltr\">The process for Terrestrial LiDAR Scanning (TLS) generally involves setting the desired scan density, which in affect increases the number of points collected and time needed per <img alt=\"\" class=\"size-medium wp-image-5462 alignleft\" decoding=\"async\" height=\"173\" loading=\"lazy\" sizes=\"(max-width: 350px) 100vw, 350px\" src=\"https://e4e.ucsd.edu/wp-content/uploads/LiDARm71insideoutside-350x173.png\" srcset=\"https://e4e.ucsd.edu/wp-content/uploads/LiDARm71insideoutside-350x173.png 350w, https://e4e.ucsd.edu/wp-content/uploads/LiDARm71insideoutside-1074x530.png 1074w, https://e4e.ucsd.edu/wp-content/uploads/LiDARm71insideoutside-768x379.png 768w, https://e4e.ucsd.edu/wp-content/uploads/LiDARm71insideoutside-1536x758.png 1536w, https://e4e.ucsd.edu/wp-content/uploads/LiDARm71insideoutside.png 1895w\" width=\"350\">scan. In the case of the Lieca BLK360 TLS, medium quality was sufficient for our purposes and took 3 minutes per scan. Each scan occurred more than 1 meter apart depending on if all the environment’s geometry could be reached from the previous scans location. The set of individual point clouds obtained was then registered using visual alignment in Lieca’s propriety software Cyclone Register 360. The output of all this is several standard file format .ptx files all in alignment with each other, which can then be brought to external software for further 3D point cloud and mesh processing.</img></p>\n",
      "ERROR: <p class=\"CDt4Ke zfr3Q\" dir=\"ltr\">The process for Terrestrial LiDAR Scanning (TLS) generally involves setting the desired scan density, which in affect increases the number of points collected and time needed per <img alt=\"\" class=\"size-medium wp-image-5462 alignleft\" decoding=\"async\" height=\"173\" loading=\"lazy\" sizes=\"(max-width: 350px) 100vw, 350px\" src=\"https://e4e.ucsd.edu/wp-content/uploads/LiDARm71insideoutside-350x173.png\" srcset=\"https://e4e.ucsd.edu/wp-content/uploads/LiDARm71insideoutside-350x173.png 350w, https://e4e.ucsd.edu/wp-content/uploads/LiDARm71insideoutside-1074x530.png 1074w, https://e4e.ucsd.edu/wp-content/uploads/LiDARm71insideoutside-768x379.png 768w, https://e4e.ucsd.edu/wp-content/uploads/LiDARm71insideoutside-1536x758.png 1536w, https://e4e.ucsd.edu/wp-content/uploads/LiDARm71insideoutside.png 1895w\" width=\"350\">scan. In the case of the Lieca BLK360 TLS, medium quality was sufficient for our purposes and took 3 minutes per scan. Each scan occurred more than 1 meter apart depending on if all the environment’s geometry could be reached from the previous scans location. The set of individual point clouds obtained was then registered using visual alignment in Lieca’s propriety software Cyclone Register 360. The output of all this is several standard file format .ptx files all in alignment with each other, which can then be brought to external software for further 3D point cloud and mesh processing.</img></p>\n",
      "ERROR: <li class=\"TYR86d wXCUfe zfr3Q\" dir=\"ltr\">\n",
      "<p class=\"CDt4Ke zfr3Q\" dir=\"ltr\">Structure from Motion (SfM)</p>\n",
      "ERROR: <p class=\"CDt4Ke zfr3Q\" dir=\"ltr\">Structure from Motion (SfM)</p>\n",
      "ERROR: <li class=\"TYR86d wXCUfe zfr3Q\" dir=\"ltr\">\n",
      "<p class=\"CDt4Ke zfr3Q\" dir=\"ltr\">SfM point cloud derived models are able to store accurate RGB data at a higher resolution, but with lower geometric accuracy. Since RGB information collected during SfM have greater accuracy, they can be useful in projecting color information back onto the Mesh generated by LIDAR. We can do Fine registration with the Iterative Closet Point (ICP) algorithm on the 2 Models to share the same coordinate space.<br/>\n",
      "ERROR: <p class=\"CDt4Ke zfr3Q\" dir=\"ltr\">SfM point cloud derived models are able to store accurate RGB data at a higher resolution, but with lower geometric accuracy. Since RGB information collected during SfM have greater accuracy, they can be useful in projecting color information back onto the Mesh generated by LIDAR. We can do Fine registration with the Iterative Closet Point (ICP) algorithm on the 2 Models to share the same coordinate space.<br/>\n",
      "['https://e4e.ucsd.edu/wp-content/uploads/sfmlidarintegeration-350x184.png']\n",
      "['https://e4e.ucsd.edu/wp-content/uploads/CaveWallScan-350x244.png', 'https://e4e.ucsd.edu/wp-content/uploads/LimestoneStuccoScan-350x244.png']\n",
      "ERROR: <li class=\"TYR86d wXCUfe zfr3Q\" dir=\"ltr\">\n",
      "<p class=\"CDt4Ke zfr3Q\" dir=\"ltr\">Aerial Light Detection and Ranging (LiDAR)</p>\n",
      "ERROR: <p class=\"CDt4Ke zfr3Q\" dir=\"ltr\">Aerial Light Detection and Ranging (LiDAR)</p>\n",
      "ERROR: <li class=\"TYR86d wXCUfe zfr3Q\" dir=\"ltr\">\n",
      "<p class=\"CDt4Ke zfr3Q\" dir=\"ltr\">The Foundation for Maya Cultural and Natural Heritage (PACUNAM) funded a LiDAR initiative that surveyed a portion of the Guatemalan jungle including the El Zotz region of Protected Biotope San Miguel La Palotada. The LiDAR rapidly emits pulses of light that transmit through multiple surface intersections. Each intersection returns a signal that captures it’s Time-of-Flight and from that a 3-dimensional coordinate and classification can be assigned to each point. The ground points can be extracted to reveal the terrain which can then be used to generate an accurate landscape than can serve as a canvas to align TLS and SfM scans to capture a broader environment.</p>\n",
      "ERROR: <p class=\"CDt4Ke zfr3Q\" dir=\"ltr\">The Foundation for Maya Cultural and Natural Heritage (PACUNAM) funded a LiDAR initiative that surveyed a portion of the Guatemalan jungle including the El Zotz region of Protected Biotope San Miguel La Palotada. The LiDAR rapidly emits pulses of light that transmit through multiple surface intersections. Each intersection returns a signal that captures it’s Time-of-Flight and from that a 3-dimensional coordinate and classification can be assigned to each point. The ground points can be extracted to reveal the terrain which can then be used to generate an accurate landscape than can serve as a canvas to align TLS and SfM scans to capture a broader environment.</p>\n",
      "<h2><strong>Deployments</strong></h2> ['Deployments']\n",
      "<p><img align=\"left\" alt=\"Guatemala\" class=\"alignleft\" decoding=\"async\" height=\"213\" loading=\"lazy\" src=\"https://c1.staticflickr.com/9/8452/29366249232_17565cab29_n.jpg\" width=\"320\"/> The Guatemala project is a collaboration between several organizations, Jason Paterniti (GEOS), Tom Garrison (USC), Edwin Roman-Ramirez (UT Austin), Ryan Kastner, Albert Lin and Curt Schurgers (UCSD, E4E, QI, NatGeo).</p>\n",
      "['https://c1.staticflickr.com/9/8452/29366249232_17565cab29_n.jpg']\n",
      "<p>The project started in 2014 with two expeditions to Guatemala. Albert, Ryan, Curt, Perry Naughton, Eric Lo, and Dominique Meyer, traveled to Guatemala in February to evaluate Quadcopters as a method of surveying jungle-bound archaeological sites. The second expedition, in May, focused on data gathering and survey techniques in several archeological sites, some of which contained Maya tombs. Curt and Ryan returned to Guatemala, and brought Dustin Richmond, David Dantas, and Sabrina Trinh. In June 2016, PhD student Quentin Gautier focused on testing remote sensing tablet technology, and in June 2017, Curt, Quentin and PhD student Peter Tueller went to the field to test a prototype of portable 3D scanner to collect data in the archaeological excavations. Starting Fall 2018 Quentin spearheaded the projects shift to VR and began reaching out to <a href=\"https://tritonxr.ucsd.edu/\">TritonXR</a> club members in order to help recreate these locations. That following Summer 2019 Quentin along with Giovanni Vindiola returned to El Zotz and interviewed our collaborators Tom Garrison (UT Austin) and Edwin Roman-Ramirez (UT Austin) for narrations, and further scanning of structures and surfaces for our VR Experience.</p>\n",
      "<h2><strong>Leaders / Contact</strong></h2> ['Leaders / Contact']\n",
      "<p><img alt=\"Guatemala\" class=\"alignright\" decoding=\"async\" height=\"213\" loading=\"lazy\" src=\"https://c1.staticflickr.com/4/3936/15266303808_2f385acc95_n.jpg\" width=\"320\"/></p>\n",
      "['https://c1.staticflickr.com/4/3936/15266303808_2f385acc95_n.jpg']\n",
      "<h2><strong>In the Press</strong></h2> ['In the Press']\n",
      "<p>Calit2 Newsroom: <a href=\"http://www.calit2.net/newsroom/article.php?id=2420\">Capturing Ancient Maya Sites from Both a Rat’s and a ‘Bat’s Eye View’</a> (Credit Tiffany Fox)</p>\n",
      "<p>International Business Times: <a href=\"http://www.ibtimes.com/drones-lasers-help-archaeologists-study-ancient-mayan-ruins-hidden-guatemala-jungle-video-1690757\">Drones, Lasers Help Archaeologists Study Ancient Mayan Ruins Hidden In Guatemala Jungle</a></p>\n",
      "<p>Phys Org: <a href=\"http://phys.org/news/2014-09-capturing-ancient-maya-sites-rat.html\">Capturing ancient Maya sites from both a rat’s and a ‘bat’s eye view’</a></p>\n",
      "\n",
      "\n",
      "#Maya Archaeology\n",
      "\n",
      "#Menu\n",
      "\n",
      "<img alt=\"Scanning Masks in Maya Temples\" class=\"alignright\" decoding=\"async\" fetchpriority=\"high\" height=\"213\" src=\"https://c3.staticflickr.com/9/8522/29008736970_a393afb9a5_n.jpg\" width=\"320\"><span style=\"font-size: inherit;\">In the dense jungle canopies of Central America and the Yucatan, much of what remains of the Maya civilization remains hidden from view. While important archaeological sites have been discovered and are now iconic tourist destinations, an understanding of how the Maya used the land – the extent of their settlements, the organization of towns and spheres of influences – is yet unknown. Currently, remote sensing technologies that can see through the thick canopy exist, but they are prohibitively expensive. Fortunately, the convergence of aerial drones and light-weight advanced sensor packages such as LiDAR, have just now opened the door to creating the disruptive technology needed to explore the hidden secrets of the Maya. One goal of this project is to assemble a remote sensing device using drone technology for the jungle.</span></img>![]({{\"assets/projects/old_projects/maya-archaeology/29008736970_a393afb9a5_n.jpg\" | absolute_url}})\n",
      "\n",
      "Already discovered Maya ruins are hidden under dense jungle. As a consequence, many sites that are not popular tourist destinations, yet instrumental in understanding Maya culture, are seen by very few people. Items found during excavation are typically claimed for preservation in museums. Large exhibits such as temple structures, by virtue of being buildings, cannot be demonstrated to a wider audience. Our goal is to change how archaeological finds are shared by exploring digital methods for documentation and visualization.\n",
      "\n",
      "This project is divided into 3 main parts:\n",
      "\n",
      " - <strong>Aerial Surveys</strong>: To fly a drone over the jungle and use LiDAR technology to map the terrain.\n",
      "\n",
      " - <strong>Tunnel Mapping</strong>: To build prototypes of hand-held 3D scanners to generate 3D models of tunnel excavations of ancient Maya temples.\n",
      "\n",
      " - <strong>Virtual Reality Visualization</strong>: To create a immersive visualization of digital models of archaeological sites and artifacts.\n",
      "\n",
      "#Aerial Surveys\n",
      "\n",
      "<div style=\"text-align: left;\"><iframe frameborder=\"0\" height=\"281\" src=\"//player.vimeo.com/video/88212266\" width=\"500\"></iframe></div>\n",
      "\n",
      "#Tunnel Mapping\n",
      "\n",
      "The Tunnel Mapping project aims to lower the cost of digital documentation by experimenting with data collection methods. These methods include stereo-panoramic cameras, LiDAR, and experimental remote sensing systems based on the Microsoft Kinect camera or the Google Tango tablet. In prior expeditions to Guatemala, we brought a ground-based LiDAR system for high resolution scans of these large excavated temples. One method we are particularly excited about is Structure from Motion (SfM), a low cost technique for generating 3D models using photos from a traditional camera. This group is seeking highly motivated individuals to build data collection infrastructure for expeditions.\n",
      "\n",
      "<div style=\"float: left; margin-right: 14px;\"><iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" src=\"https://www.youtube.com/embed/7tKovc0Eo54\" width=\"540\"></iframe></div>\n",
      "\n",
      "<iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/pTD_uXEwNHw\" width=\"540\"></iframe><p><iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"https://www.youtube.com/embed/pTD_uXEwNHw\" width=\"540\"></iframe></p>\n",
      "\n",
      "Below, is a fly-through video created from a composite point cloud generated from 50 LIDAR scans.\n",
      "\n",
      "<div style=\"text-align: center;\"><iframe frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"//www.youtube.com/embed/M6AKUE79xWw\" width=\"560\"></iframe></div>\n",
      "\n",
      "This video is of a point cloud generated by taking pictures of a stucco mask inside of an excavated temple on the site.\n",
      "\n",
      "<div style=\"text-align: center;\"><iframe frameborder=\"0\" height=\"315\" loading=\"lazy\" src=\"//www.youtube.com/embed/g2pv-pOSjyo\" width=\"560\"></iframe></div>\n",
      "\n",
      "#Virtual Reality Visualization\n",
      "\n",
      "ERROR: <p style=\"text-align: center;\"><strong><em><a href=\"https://sites.google.com/ucsd.edu/maya-archeology-xr/home\">Maya Archaeology VR Website</a></em></strong></p>\n",
      "\n",
      "We seek to expand distribution by creating immersive visualizations of the many 3D models that we have collected over the years. We are looking for any person motivated by VR environments, video game creation, 3D modeling, and 3D point cloud manipulation.\n",
      "\n",
      "<div style=\"padding: 56.25% 0 0 0; position: relative;\"><iframe allowfullscreen=\"allowfullscreen\" frameborder=\"0\" src=\"https://player.vimeo.com/video/364134407\" style=\"position: absolute; top: 0; left: 0; width: 100%; height: 100%;\"></iframe></div>\n",
      "\n",
      "<script src=\"https://player.vimeo.com/api/player.js\"></script>\n",
      "\n",
      "ERROR: <p>Visualization of Structure M7-1 in El Zotz Guatemala.<br/>\n",
      "\n",
      "ERROR: <p>Created using Point clouds from the Faro Focus 3D 120, Lieca BLK360.<br/>\n",
      "\n",
      "![]({{\"assets/projects/old_projects/maya-archaeology/SpiderMonkeyBowlUnrealEngine-350x255.png\" | absolute_url}})\n",
      "\n",
      "#Approach\n",
      "\n",
      "#Data Acquisition through Remote Scanning\n",
      "\n",
      "ERROR: <li class=\"TYR86d wXCUfe zfr3Q\" dir=\"ltr\">\n",
      "\n",
      "ERROR: <p class=\"CDt4Ke zfr3Q\" dir=\"ltr\">Terrestrial Light Detection and Ranging (LiDAR)</p>\n",
      "\n",
      "ERROR: <li class=\"TYR86d wXCUfe zfr3Q\" dir=\"ltr\">\n",
      "\n",
      "ERROR: <p class=\"CDt4Ke zfr3Q\" dir=\"ltr\">The process for Terrestrial LiDAR Scanning (TLS) generally involves setting the desired scan density, which in affect increases the number of points collected and time needed per <img alt=\"\" class=\"size-medium wp-image-5462 alignleft\" decoding=\"async\" height=\"173\" loading=\"lazy\" sizes=\"(max-width: 350px) 100vw, 350px\" src=\"https://e4e.ucsd.edu/wp-content/uploads/LiDARm71insideoutside-350x173.png\" srcset=\"https://e4e.ucsd.edu/wp-content/uploads/LiDARm71insideoutside-350x173.png 350w, https://e4e.ucsd.edu/wp-content/uploads/LiDARm71insideoutside-1074x530.png 1074w, https://e4e.ucsd.edu/wp-content/uploads/LiDARm71insideoutside-768x379.png 768w, https://e4e.ucsd.edu/wp-content/uploads/LiDARm71insideoutside-1536x758.png 1536w, https://e4e.ucsd.edu/wp-content/uploads/LiDARm71insideoutside.png 1895w\" width=\"350\">scan. In the case of the Lieca BLK360 TLS, medium quality was sufficient for our purposes and took 3 minutes per scan. Each scan occurred more than 1 meter apart depending on if all the environment’s geometry could be reached from the previous scans location. The set of individual point clouds obtained was then registered using visual alignment in Lieca’s propriety software Cyclone Register 360. The output of all this is several standard file format .ptx files all in alignment with each other, which can then be brought to external software for further 3D point cloud and mesh processing.</img></p>\n",
      "\n",
      "ERROR: <li class=\"TYR86d wXCUfe zfr3Q\" dir=\"ltr\">\n",
      "\n",
      "ERROR: <p class=\"CDt4Ke zfr3Q\" dir=\"ltr\">Structure from Motion (SfM)</p>\n",
      "\n",
      "ERROR: <li class=\"TYR86d wXCUfe zfr3Q\" dir=\"ltr\">\n",
      "\n",
      "ERROR: <p class=\"CDt4Ke zfr3Q\" dir=\"ltr\">SfM point cloud derived models are able to store accurate RGB data at a higher resolution, but with lower geometric accuracy. Since RGB information collected during SfM have greater accuracy, they can be useful in projecting color information back onto the Mesh generated by LIDAR. We can do Fine registration with the Iterative Closet Point (ICP) algorithm on the 2 Models to share the same coordinate space.<br/>\n",
      "\n",
      "![]({{\"assets/projects/old_projects/maya-archaeology/sfmlidarintegeration-350x184.png\" | absolute_url}})\n",
      "\n",
      "![]({{\"assets/projects/old_projects/maya-archaeology/CaveWallScan-350x244.png\" | absolute_url}})\n",
      "\n",
      "![]({{\"assets/projects/old_projects/maya-archaeology/LimestoneStuccoScan-350x244.png\" | absolute_url}})\n",
      "\n",
      "ERROR: <li class=\"TYR86d wXCUfe zfr3Q\" dir=\"ltr\">\n",
      "\n",
      "ERROR: <p class=\"CDt4Ke zfr3Q\" dir=\"ltr\">Aerial Light Detection and Ranging (LiDAR)</p>\n",
      "\n",
      "ERROR: <li class=\"TYR86d wXCUfe zfr3Q\" dir=\"ltr\">\n",
      "\n",
      "ERROR: <p class=\"CDt4Ke zfr3Q\" dir=\"ltr\">The Foundation for Maya Cultural and Natural Heritage (PACUNAM) funded a LiDAR initiative that surveyed a portion of the Guatemalan jungle including the El Zotz region of Protected Biotope San Miguel La Palotada. The LiDAR rapidly emits pulses of light that transmit through multiple surface intersections. Each intersection returns a signal that captures it’s Time-of-Flight and from that a 3-dimensional coordinate and classification can be assigned to each point. The ground points can be extracted to reveal the terrain which can then be used to generate an accurate landscape than can serve as a canvas to align TLS and SfM scans to capture a broader environment.</p>\n",
      "\n",
      "#Deployments\n",
      "\n",
      "<img align=\"left\" alt=\"Guatemala\" class=\"alignleft\" decoding=\"async\" height=\"213\" loading=\"lazy\" src=\"https://c1.staticflickr.com/9/8452/29366249232_17565cab29_n.jpg\" width=\"320\"/> The Guatemala project is a collaboration between several organizations, Jason Paterniti (GEOS), Tom Garrison (USC), Edwin Roman-Ramirez (UT Austin), Ryan Kastner, Albert Lin and Curt Schurgers (UCSD, E4E, QI, NatGeo).![]({{\"assets/projects/old_projects/maya-archaeology/29366249232_17565cab29_n.jpg\" | absolute_url}})\n",
      "\n",
      "The project started in 2014 with two expeditions to Guatemala. Albert, Ryan, Curt, Perry Naughton, Eric Lo, and Dominique Meyer, traveled to Guatemala in February to evaluate Quadcopters as a method of surveying jungle-bound archaeological sites. The second expedition, in May, focused on data gathering and survey techniques in several archeological sites, some of which contained Maya tombs. Curt and Ryan returned to Guatemala, and brought Dustin Richmond, David Dantas, and Sabrina Trinh. In June 2016, PhD student Quentin Gautier focused on testing remote sensing tablet technology, and in June 2017, Curt, Quentin and PhD student Peter Tueller went to the field to test a prototype of portable 3D scanner to collect data in the archaeological excavations. Starting Fall 2018 Quentin spearheaded the projects shift to VR and began reaching out to <a href=\"https://tritonxr.ucsd.edu/\">TritonXR</a> club members in order to help recreate these locations. That following Summer 2019 Quentin along with Giovanni Vindiola returned to El Zotz and interviewed our collaborators Tom Garrison (UT Austin) and Edwin Roman-Ramirez (UT Austin) for narrations, and further scanning of structures and surfaces for our VR Experience.\n",
      "\n",
      "#Leaders / Contact\n",
      "\n",
      "<img alt=\"Guatemala\" class=\"alignright\" decoding=\"async\" height=\"213\" loading=\"lazy\" src=\"https://c1.staticflickr.com/4/3936/15266303808_2f385acc95_n.jpg\" width=\"320\"/>![]({{\"assets/projects/old_projects/maya-archaeology/15266303808_2f385acc95_n.jpg\" | absolute_url}})\n",
      "\n",
      " - Nathan Hui (<a href=\"javascript:DeCryptX('2p3w1i0u3l1A3h1o1h0.0u3f2u0d310e2f2w')\">nthui@eng.ucsd.edu</a>)\n",
      "\n",
      "#In the Press\n",
      "\n",
      "Calit2 Newsroom: <a href=\"http://www.calit2.net/newsroom/article.php?id=2420\">Capturing Ancient Maya Sites from Both a Rat’s and a ‘Bat’s Eye View’</a> (Credit Tiffany Fox)\n",
      "\n",
      "International Business Times: <a href=\"http://www.ibtimes.com/drones-lasers-help-archaeologists-study-ancient-mayan-ruins-hidden-guatemala-jungle-video-1690757\">Drones, Lasers Help Archaeologists Study Ancient Mayan Ruins Hidden In Guatemala Jungle</a>\n",
      "\n",
      "Phys Org: <a href=\"http://phys.org/news/2014-09-capturing-ancient-maya-sites-rat.html\">Capturing ancient Maya sites from both a rat’s and a ‘bat’s eye view’</a>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = convert_to_markdown( \"../../assets/projects-old_projects-maya-archaeology-\", \"https://e4e.ucsd.edu/maya-archaeology\")\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 class=\"entry-title\">Angry Birds</h1> ['Angry Birds']\n",
      "<!--<h1 class=\"menu-toggle\">Menu</h1>--> ['Menu']\n",
      "ERROR: <li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-3916\" id=\"menu-item-3916\"><a href=\"https://e4e.ucsd.edu/angry-birds/media\">Media</a></li>\n",
      "ERROR: <li class=\"menu-item menu-item-type-post_type menu-item-object-page current-menu-item page_item page-item-1977 current_page_item menu-item-3915\" id=\"menu-item-3915\"><a aria-current=\"page\" href=\"https://e4e.ucsd.edu/angry-birds\">Angry Birds</a></li>\n",
      "<p>The autonomous bird collision monitor (fondly named Angry Birds) is a simplistic and compact ecological research tool for biologists studying bird strikes against plate glass. This is a critical issue when birds are drawn towards glass reflections, which deceptively capture the natural scenery of its background, leading to a high number of in flight mortality.  The problem is an especially endemic concern as San Diego is located in the path of a major flyway for migratory birds.</p>\n",
      "['https://e4e.ucsd.edu/wordpress/wp-content/uploads/dead-birds.jpg']\n",
      "<p>The Angry Birds project will not only strive to provide valuable biological footage and data, but it will aid the San Diego Zoos Institute for Conservation Research in monitoring and assessing the effectiveness of a new preventive UV film technology (in development) aimed at deterring bird collisions against plate glass.</p>\n",
      "['https://e4e.ucsd.edu/wordpress/wp-content/uploads/Angry-Birds-front-cropped2.png']\n",
      "<p>Using a series of piezoelectric sensors mounted onto glass, the monitor will be able to detect vibrations and determine whether a collision has occurred. Once a collision has been detected, the system will then proceed to store data and video footage belonging to that event in order to further avian research by biologists. Our detection system will be a significant technical improvement for researchers seeking to quantify an accurate number of collisions over conventional retrieve-and-count methods which provide only rough estimates at best.</p>\n",
      "<p>Please contact our staff engineer Nathan Hui (<a href=\"javascript:DeCryptX('3q1u3k2w2k2B3h1o3j203x0c3v0d1/3h0d3x')\">nthui@eng.ucsd.edu</a>) for more information</p>\n",
      "\n",
      "\n",
      "# Angry Birds\n",
      "\n",
      "# Menu\n",
      "\n",
      "ERROR: <li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-3916\" id=\"menu-item-3916\"><a href=\"https://e4e.ucsd.edu/angry-birds/media\">Media</a></li>\n",
      "\n",
      "ERROR: <li class=\"menu-item menu-item-type-post_type menu-item-object-page current-menu-item page_item page-item-1977 current_page_item menu-item-3915\" id=\"menu-item-3915\"><a aria-current=\"page\" href=\"https://e4e.ucsd.edu/angry-birds\">Angry Birds</a></li>\n",
      "\n",
      "The autonomous bird collision monitor (fondly named Angry Birds) is a simplistic and compact ecological research tool for biologists studying bird strikes against plate glass. This is a critical issue when birds are drawn towards glass reflections, which deceptively capture the natural scenery of its background, leading to a high number of in flight mortality.  The problem is an especially endemic concern as San Diego is located in the path of a major flyway for migratory birds.\n",
      "\n",
      "![]({{\"assets/old_projects-projects-angry-birds-dead-birds.jpg\" | absolute_url}})\n",
      "\n",
      "The Angry Birds project will not only strive to provide valuable biological footage and data, but it will aid the San Diego Zoos Institute for Conservation Research in monitoring and assessing the effectiveness of a new preventive UV film technology (in development) aimed at deterring bird collisions against plate glass.\n",
      "\n",
      "![]({{\"assets/old_projects-projects-angry-birds-Angry-Birds-front-cropped2.png\" | absolute_url}})\n",
      "\n",
      "Using a series of piezoelectric sensors mounted onto glass, the monitor will be able to detect vibrations and determine whether a collision has occurred. Once a collision has been detected, the system will then proceed to store data and video footage belonging to that event in order to further avian research by biologists. Our detection system will be a significant technical improvement for researchers seeking to quantify an accurate number of collisions over conventional retrieve-and-count methods which provide only rough estimates at best.\n",
      "\n",
      "Please contact our staff engineer Nathan Hui (<a href=\"javascript:DeCryptX('3q1u3k2w2k2B3h1o3j203x0c3v0d1/3h0d3x')\">nthui@eng.ucsd.edu</a>) for more information\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file = convert_to_markdown(\"../../assets/projects-old_projects-angry-birds-\", \"https://e4e.ucsd.edu/angry-birds\")\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# Angry Birds\n",
      "\n",
      "# Menu\n",
      "\n",
      "ERROR: <li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-3916\" id=\"menu-item-3916\"><a href=\"https://e4e.ucsd.edu/angry-birds/media\">Media</a></li>\n",
      "\n",
      "ERROR: <li class=\"menu-item menu-item-type-post_type menu-item-object-page current-menu-item page_item page-item-1977 current_page_item menu-item-3915\" id=\"menu-item-3915\"><a aria-current=\"page\" href=\"https://e4e.ucsd.edu/angry-birds\">Angry Birds</a></li>\n",
      "\n",
      "The autonomous bird collision monitor (fondly named Angry Birds) is a simplistic and compact ecological research tool for biologists studying bird strikes against plate glass. This is a critical issue when birds are drawn towards glass reflections, which deceptively capture the natural scenery of its background, leading to a high number of in flight mortality.  The problem is an especially endemic concern as San Diego is located in the path of a major flyway for migratory birds.\n",
      "\n",
      "![]({{\"assets/old_projects-projects-angry-birds-dead-birds.jpg\" | absolute_url}})\n",
      "\n",
      "The Angry Birds project will not only strive to provide valuable biological footage and data, but it will aid the San Diego Zoos Institute for Conservation Research in monitoring and assessing the effectiveness of a new preventive UV film technology (in development) aimed at deterring bird collisions against plate glass.\n",
      "\n",
      "![]({{\"assets/old_projects-projects-angry-birds-Angry-Birds-front-cropped2.png\" | absolute_url}})\n",
      "\n",
      "Using a series of piezoelectric sensors mounted onto glass, the monitor will be able to detect vibrations and determine whether a collision has occurred. Once a collision has been detected, the system will then proceed to store data and video footage belonging to that event in order to further avian research by biologists. Our detection system will be a significant technical improvement for researchers seeking to quantify an accurate number of collisions over conventional retrieve-and-count methods which provide only rough estimates at best.\n",
      "\n",
      "Please contact our staff engineer Nathan Hui (<a href=\"javascript:DeCryptX('3q1u3k2w2k2B3h1o3j203x0c3v0d1/3h0d3x')\">nthui@eng.ucsd.edu</a>) for more information\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - assets/old_projects-projects-angry-birds-16562562929_cf7b331d3b.jpg\n",
      " - assets/old_projects-projects-angry-birds-16747478571_c8481be310.jpg\n",
      " - assets/old_projects-projects-angry-birds-16748659635_409785b276.jpg\n",
      " - assets/old_projects-projects-angry-birds-16568177890_01c0dc7a82.jpg\n",
      " - assets/old_projects-projects-angry-birds-16548318617_4fb46dfa79.jpg\n",
      " - assets/old_projects-projects-angry-birds-16755642815_635a9e5a32.jpg\n",
      " - assets/old_projects-projects-angry-birds-16729770616_15f1265a38.jpg\n",
      " - assets/old_projects-projects-angry-birds-16754664522_452f2ea434.jpg\n",
      " - assets/old_projects-projects-angry-birds-16755784535_495f1152ab.jpg\n",
      " - assets/old_projects-projects-angry-birds-16729971536_1a0268366f.jpg\n",
      " - assets/old_projects-projects-angry-birds-16755952205_15e2319ac0.jpg\n",
      " - assets/old_projects-projects-angry-birds-16569866189_08531ab581.jpg\n",
      " - assets/old_projects-projects-angry-birds-16569156410_14ffe093d5.jpg\n",
      " - assets/old_projects-projects-angry-birds-16569290310_2e94d442c3.jpg\n",
      " - assets/old_projects-projects-angry-birds-16569153798_e9b08653e0.jpg\n",
      " - assets/old_projects-projects-angry-birds-16570652619_40ec41daa0.jpg\n",
      " - assets/old_projects-projects-angry-birds-16134462924_592bb54987.jpg\n",
      " - assets/old_projects-projects-angry-birds-16730875856_8a163dcab8.jpg\n",
      " - assets/old_projects-projects-angry-birds-16134458454_90ae6b441e.jpg\n",
      " - assets/old_projects-projects-angry-birds-16569199508_35ba80f607.jpg\n",
      " - assets/old_projects-projects-angry-birds-16756766365_19480b0629.jpg\n",
      " - assets/old_projects-projects-angry-birds-16755592901_2e56b73910.jpg\n",
      " - assets/old_projects-projects-angry-birds-16755592111_db9248cf1b.jpg\n",
      " - assets/old_projects-projects-angry-birds-16570640619_3a42b128cf.jpg\n",
      " - assets/old_projects-projects-angry-birds-16755884291_e84ed30df4.jpg\n",
      " - assets/old_projects-projects-angry-birds-16549791297_aba62b3098.jpg\n",
      " - assets/old_projects-projects-angry-birds-16137110833_16a79a087b.jpg\n",
      " - assets/old_projects-projects-angry-birds-16549789887_26915a0d51.jpg\n",
      " - assets/old_projects-projects-angry-birds-16757048975_18c79e15ee.jpg\n",
      " - assets/old_projects-projects-angry-birds-16755880201_05e2d4c21f.jpg\n",
      " - assets/old_projects-projects-angry-birds-16755872151_a38a596d7e.jpg\n",
      " - assets/old_projects-projects-angry-birds-16134743634_fd15460503.jpg\n",
      " - assets/old_projects-projects-angry-birds-16569480328_2f8cd39525.jpg\n",
      " - assets/old_projects-projects-angry-birds-16570926799_d8af15ef4b.jpg\n",
      " - assets/old_projects-projects-angry-birds-16755981042_0b9d076732.jpg\n",
      " - assets/old_projects-projects-angry-birds-16134740004_46b4a775ee.jpg\n",
      " - assets/old_projects-projects-angry-birds-16549783797_6ffd40e0f4.jpg\n",
      " - assets/old_projects-projects-angry-birds-16755874611_01076932cd.jpg\n",
      " - assets/old_projects-projects-angry-birds-16134738564_dbd6c1ff43.jpg\n",
      " - assets/old_projects-projects-angry-birds-16569647510_0825a10883.jpg\n",
      " - assets/old_projects-projects-angry-birds-16549781307_7838ee2330.jpg\n",
      " - assets/old_projects-projects-angry-birds-16757040315_67a2c97c4f.jpg\n"
     ]
    }
   ],
   "source": [
    "download_images(\"../../assets/projects-old_projects-angry-birds-\", \"https://e4e.ucsd.edu/angry-birds/media\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(asset_folder, og_webpage):\n",
    "    request = requests.get(og_webpage)\n",
    "    soup = BeautifulSoup(request.text)\n",
    "    images = soup.find(id=\"primary\").find_all(\"img\")\n",
    "    \n",
    "    asset_folder_md = asset_folder.replace(\"../\", \"\")\n",
    "    for image in images:\n",
    "        src = image[\"src\"]\n",
    "        filename = src.split(\"/\")[-1]\n",
    "        print(' - ' +asset_folder_md + filename) \n",
    "        #print('![]({{' +asset_folder_md + filename+ ' | absolute_url}}') \n",
    "\n",
    "        data = requests.get(src).content \n",
    "        \n",
    "        # Opening a new file named img with extension .jpg \n",
    "        # This file would store the data of the image file \n",
    "        f = open(asset_folder + filename,'wb') \n",
    "        \n",
    "        # Storing the image data inside the data variable to the file \n",
    "        f.write(data) \n",
    "        f.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BREAK FOR CLEAN CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 class=\"entry-title\">Underwater Cave Imaging</h1> ['Underwater Cave Imaging']\n",
      "<!--<h1 class=\"menu-toggle\">Menu</h1>--> ['Menu']\n",
      "ERROR: <li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-4188\" id=\"menu-item-4188\"><a href=\"https://e4e.ucsd.edu/underwater-cave-imaging/media\">Media</a></li>\n",
      "ERROR: <li class=\"menu-item menu-item-type-post_type menu-item-object-page current-menu-item page_item page-item-4059 current_page_item menu-item-4187\" id=\"menu-item-4187\"><a aria-current=\"page\" href=\"https://e4e.ucsd.edu/underwater-cave-imaging\">Underwater Cave Imaging</a></li>\n",
      "<p>We are collaborating with Dr. Alfie Rosenberger from Brooklyn College and Fabio Amador from National Geographic to develop technology to create immersive imagery of underwater caves. The video below describes our first deployment site:</p>\n",
      "<p><iframe allowfullscreen=\"\" class=\"youtube-center\" frameborder=\"0\" height=\"315\" src=\"https://www.youtube.com/embed/QX46qzzHGNI\" width=\"560\"></iframe></p>\n",
      "<p>We are developing a 3D Visualization System for the cave divers to use in the field.</p>\n"
     ]
    }
   ],
   "source": [
    "url = \"https://e4e.ucsd.edu/underwater-cave-imaging\"\n",
    "photo_url = \"../../assets/projects/old_projects/underwater-cave-imaging/\"\n",
    "file = convert_to_markdown(photo_url, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "# Underwater Cave Imaging\n",
      "\n",
      "# Menu\n",
      "\n",
      "ERROR: <li class=\"menu-item menu-item-type-post_type menu-item-object-page menu-item-4188\" id=\"menu-item-4188\"><a href=\"https://e4e.ucsd.edu/underwater-cave-imaging/media\">Media</a></li>\n",
      "\n",
      "ERROR: <li class=\"menu-item menu-item-type-post_type menu-item-object-page current-menu-item page_item page-item-4059 current_page_item menu-item-4187\" id=\"menu-item-4187\"><a aria-current=\"page\" href=\"https://e4e.ucsd.edu/underwater-cave-imaging\">Underwater Cave Imaging</a></li>\n",
      "\n",
      "We are collaborating with Dr. Alfie Rosenberger from Brooklyn College and Fabio Amador from National Geographic to develop technology to create immersive imagery of underwater caves. The video below describes our first deployment site:\n",
      "\n",
      "<iframe allowfullscreen=\"\" class=\"youtube-center\" frameborder=\"0\" height=\"315\" src=\"https://www.youtube.com/embed/QX46qzzHGNI\" width=\"560\"></iframe><p><iframe allowfullscreen=\"\" class=\"youtube-center\" frameborder=\"0\" height=\"315\" src=\"https://www.youtube.com/embed/QX46qzzHGNI\" width=\"560\"></iframe></p>\n",
      "\n",
      "We are developing a 3D Visualization System for the cave divers to use in the field.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_images(photo_url, url +\"/media\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
